---
# yamllint disable
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  # The name will be used to generate the placementBinding and placementRule names as {name}-placementBinding and {name}-placementRule
  name: "group-smcx12-vse2"
  namespace: "ztp-group"
spec:
  bindingRules:
    # These policies will correspond to all clusters with this label:
    group-smcx12-vse2: ""
  mcp: "master"
  sourceFiles:
    - fileName: AcceleratorsNS.yaml
      policyName: "subscriptions-policy"
    - fileName: AcceleratorsOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: AcceleratorsSubscription.yaml
      policyName: "subscriptions-policy"
    - fileName: ClusterLogging.yaml
      policyName: "config-policy"
      spec:
        collection:
          type: vector
        curation:
          curator:
            schedule: 30 3 * * *  
    # The setting below overrides the default "worker" selector predefined in
    # the source-crs. The change is recommended on SNOs configured with PTP 
    # event notification for forward compatibility with possible SNO expansion.
    # When the default setting is left intact, then in case of an SNO 
    # expansion with one or more workers, PTP operator
    # would not create linuxptp-daemon containers on the worker node(s). Any
    # attempt to change the daemonsetNodeSelector will result in ptp daemon
    # restart and time synchronization loss.
    # After complying with the policy, complianceType can be set to a safer "musthave"
    # - fileName: PtpOperatorConfigForEvent.yaml
    #   policyName: "config-policy"
    #   complianceType: mustonlyhave
    #   spec:
    #     daemonNodeSelector:
    #       node-role.kubernetes.io/worker: ""
    #     ptpEventConfig:
    #       storageType: "storage-class-http-events"
    - fileName: PtpConfigGmWpc.yaml
      policyName: "config-policy"
      spec:
        profile:
        - name: "grandmaster"
          phc2sysOpts: -r -u 0 -m -O -37 -N 8 -R 16 -s ens6f0 -n 24
          ptp4lConf: |
            [ens6f0]
            masterOnly 1
            [global]
            #
            # Default Data Set
            #
            twoStepFlag 1
            slaveOnly 0
            priority1 128
            priority2 128
            domainNumber 24
            #utc_offset 37
            clockClass 248
            clockAccuracy 0xFE
            offsetScaledLogVariance 0xFFFF
            free_running 0
            freq_est_interval 1
            dscp_event 0
            dscp_general 0
            #dataset_comparison ieee1588
            dataset_comparison G.8275.x
            G.8275.defaultDS.localPriority 128
            #
            # Port Data Set
            #
            logAnnounceInterval -3
            logSyncInterval -4
            logMinDelayReqInterval -4
            logMinPdelayReqInterval -4
            announceReceiptTimeout 3
            syncReceiptTimeout 0
            delayAsymmetry 0
            fault_reset_interval 4
            neighborPropDelayThresh 20000000
            masterOnly 1
            G.8275.portDS.localPriority 128
            #
            # Run time options
            #
            assume_two_step 0
            logging_level 6
            path_trace_enabled 0
            follow_up_info 0
            hybrid_e2e 0
            inhibit_multicast_service 0
            net_sync_monitor 0
            tc_spanning_tree 0
            tx_timestamp_timeout 50
            unicast_listen 0
            unicast_master_table 0
            unicast_req_duration 3600
            use_syslog 1
            verbose 1
            summary_interval 0
            kernel_leap 1
            check_fup_sync 0
            #
            # Servo Options
            #
            pi_proportional_const 0.0
            pi_integral_const 0.0
            pi_proportional_scale 0.0
            pi_proportional_exponent -0.3
            pi_proportional_norm_max 0.7
            pi_integral_scale 0.0
            pi_integral_exponent 0.4
            pi_integral_norm_max 0.3
            step_threshold 0.0
            first_step_threshold 0.00002
            max_frequency 900000000
            #clock_servo pi
            clock_servo linreg
            sanity_freq_limit 200000000
            ntpshm_segment 0
            #
            # Transport options
            #
            transportSpecific 0x0
            ptp_dst_mac 01:1B:19:00:00:00
            p2p_dst_mac 01:80:C2:00:00:0E
            udp_ttl 1
            udp6_scope 0x0E
            uds_address /var/run/ptp4l
            #
            # Default interface options
            #
            clock_type OC
            network_transport L2
            delay_mechanism E2E
            time_stamping hardware
            tsproc_mode filter
            delay_filter moving_median
            delay_filter_length 10
            egressLatency 0
            ingressLatency 0
            boundary_clock_jbod 0
            #
            # Clock description
            #
            productDescription ;;
            revisionData ;;
            manufacturerIdentity 00:00:00
            userDescription ;
            timeSource 0xA0
          ts2phcConf: |
            [nmea]
            ts2phc.master 1
            [global]
            use_syslog  0
            verbose 1
            logging_level 7
            ts2phc.pulsewidth 100000000
            #GNSS module s /dev/ttyGNSS* -al use _0
            #cat /dev/ttyGNSS_1700_0 to find available serial port
            #example value of gnss_serialport is /dev/ttyGNSS_1700_0
            ts2phc.nmea_serialport /dev/gnss0
            leapfile  /usr/share/zoneinfo/leap-seconds.list
            [ens6f0]
            ts2phc.extts_polarity rising
            ts2phc.extts_correction 0
#    - fileName: PtpConfigSlave.yaml   # Change to PtpConfigSlaveCvl.yaml for ColumbiaVille NIC
#      policyName: "config-policy"
#      metadata:
#        name: "du-ptp-slave"
#      spec:
#        profile:
#        - name: "slave"
          # This interface must match the hardware in this group
#          interface: "ens5f0"
#          ptp4lOpts: "-2 -s --summary_interval -4"
#          phc2sysOpts: "-a -r -n 24"
    - fileName: SriovFecClusterConfig.yaml
      policyName: "fec-policy"
      spec:
        drainSkip: true
        acceleratorSelector:
          pciAddress: 0000:c3:00.0
        physicalFunction:
         bbDevConfig:
          acc100:
            # Programming mode: 0 = VF Programming, 1 = PF Programming
            pfMode: false
            numVfBundles: 16
            maxQueueSize: 1024
            uplink4G:
              numQueueGroups: 0
              numAqsPerGroups: 16
              aqDepthLog2: 4
            downlink4G:
              numQueueGroups: 0
              numAqsPerGroups: 16
              aqDepthLog2: 4
            uplink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
            downlink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
    - fileName: SriovOperatorConfig.yaml
      policyName: "config-policy"
      # For existing clusters with node selector set as "master", 
      # change the complianceType to "mustonlyhave".
      # After complying with the policy, the complianceType can 
      # be reverted to "musthave"
      complianceType: musthave
      spec:
        configDaemonNodeSelector:
          node-role.kubernetes.io/worker: ""
#    - fileName: StorageLV.yaml
#      policyName: "config-policy"
#      spec:
#        storageClassDevices:
#        - storageClassName: "example-storage-class-1"
#          volumeMode: Filesystem
#          fsType: xfs
#          devicePaths:
#          - /dev/sdb1
#        - storageClassName: "example-storage-class-2"
#          volumeMode: Filesystem
#          fsType: xfs
#          devicePaths:
#          - /dev/sdb2
        # This is required if PTP and BMER operators use HTTP transport.
        # The disk labels are created by ignitionConfigOverride in siteConfig.
        # - storageClassName: "storage-class-http-events"
        #   volumeMode: Filesystem
        #   fsType: xfs
        #   devicePaths:
        #   - /dev/disk/by-partlabel/httpevent1
        #   - /dev/disk/by-partlabel/httpevent2
    - fileName: DisableSnoNetworkDiag.yaml
      policyName: "config-policy"
    - fileName: PerformanceProfile.yaml
      policyName: "config-policy"
      spec:
        cpu:
          # These must be tailored for the specific hardware platform
          isolated: "2-31,34-63"
          reserved: "0-1,32-33"
        hugepages:
          defaultHugepagesSize: 1G
          pages:
            - size: 1G
              count: 16
        workloadHints:
          realTime: true
          highPowerConsumption: false
          perPodPowerManagement: false
    - fileName: TunedPerformancePatch.yaml
      policyName: "config-policy"
      spec:
        profile:
          - name: performance-patch
            data: |
              [main]
              summary=Configuration changes profile inherited from performance created tuned
              include=openshift-node-performance-openshift-node-performance-profile
              [sysctl]
              kernel.timer_migration=1
              [scheduler]
              group.ice-ptp=0:f:10:*:ice-ptp.*
              group.ice-gnss=0:f:10:*:ice-gnss.*
              [service]
              service.stalld=start,enable
              service.chronyd=stop,disable
    # # AmqInstance is required if PTP and BMER operators use AMQP transport
    # - fileName: AmqInstance.yaml
    #   policyName: "config-events-policy"
    #   annotations:
    #     ran.openshift.io/ztp-deploy-wave: "10"
    # - fileName: HardwareEvent.yaml
    #   policyName: "config-events-policy"
    #   spec:
    #     nodeSelector: {}
    #     transportHost: "amqp://amq-router.amq-router.svc.cluster.local"
    #     # transportHost: "http://hw-event-publisher-service.openshift-bare-metal-events.svc.cluster.local:9043"
    #     # storageType: "storage-class-http-events"
    #     logLevel: "debug"
    #
    # These CRs are to enable crun on master and worker nodes for 4.13+ only
    #
    # Include these CRs in the group PGT instead of the common PGT to make sure
    # they are applied after the operators have been successfully installed,
    # however, it's strongly recommended to include these CRs as day-0 extra manifests
    # to avoid the risk of an extra reboot.
    - fileName: optional-extra-manifest/enable-crun-master.yaml
      policyName: "config-policy"
    - fileName: optional-extra-manifest/enable-crun-worker.yaml
      policyName: "config-policy"
#    --- sources needed for image registry (check ImageRegistry.md for more details)----
#    - fileName: StorageClass.yaml
#      policyName: "config-policy"
#      metadata:
#        name: image-registry-sc
#    - fileName: StoragePVC.yaml
#      policyName: "config-policy"
#      metadata:
#        name: image-registry-pvc
#        namespace: openshift-image-registry
#      spec:
#        accessModes:
#          - ReadWriteMany
#        resources:
#          requests:
#            storage: 100Gi
#        storageClassName: image-registry-sc
#        volumeMode: Filesystem
#    - fileName: ImageRegistryPV.yaml  # this is assuming that mount_point is set to `/var/imageregistry` in SiteConfig
                                       # using StorageClass `image-registry-sc` (see the first sc-file)
#      policyName: "config-policy"
#    - fileName: ImageRegistryConfig.yaml
#      policyName: "config-policy"
#      spec:
#        storage:
#          pvc:
#            claim: "image-registry-pvc"
#     ---- sources needed for image registry ends here ----

#    --- sources needed for updating CRI-O workload-partitioning ----
#      more info here: on the base64 content https://docs.openshift.com/container-platform/4.11/scalability_and_performance/sno-du-enabling-workload-partitioning-on-single-node-openshift.html
#    - fileName: MachineConfigGeneric.yaml
#      policyName: "config-policy"
#      complianceType: mustonlyhave # This is to update array entry as opposed to appending a new entry.
#      metadata:
#        name: 02-master-workload-partitioning
#      spec:
#        config:
#          storage:
#            files:
#              - contents:
#                  # crio cpuset config goes below. This value needs to updated and matched PerformanceProfile. Check the link for more info on the content.
#                  source: data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS53b3JrbG9hZHMubWFuYWdlbWVudF0KYWN0aXZhdGlvbl9hbm5vdGF0aW9uID0gInRhcmdldC53b3JrbG9hZC5vcGVuc2hpZnQuaW8vbWFuYWdlbWVudCIKYW5ub3RhdGlvbl9wcmVmaXggPSAicmVzb3VyY2VzLndvcmtsb2FkLm9wZW5zaGlmdC5pbyIKcmVzb3VyY2VzID0geyAiY3B1c2hhcmVzIiA9IDAsICJjcHVzZXQiID0gIjAtMSwyLTMiIH0=
#                mode: 420
#                overwrite: true
#                path: /etc/crio/crio.conf.d/01-workload-partitioning
#                user:
#                  name: root
#              - contents:
#                  # openshift cpuset config goes below. This value needs to be updated and matched with crio cpuset (array entry above this). Check the link for more info on the content.
#                  source: data:text/plain;charset=utf-8;base64,ewogICJtYW5hZ2VtZW50IjogewogICAgImNwdXNldCI6ICIwLTEsNTItNTMiCiAgfQp9Cg==
#                mode: 420
#                overwrite: true
#                path: /etc/kubernetes/openshift-workload-pinning
#                user:
#                  name: root
#     ---- sources needed for updating CRI-O workload-partitioning ends here ----
# yamllint enable
